{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGxgR2n7Gq8B20GRSAn0tm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal7017/GoogleColaboratory2/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHGBeVDIUJXe",
        "outputId": "589a1c9c-2e9b-45da-9030-e0c4ccd48c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: error = 9.000000\n",
            "10: error = 1.549824\n",
            "20: error = 0.960131\n",
            "30: error = 0.718334\n",
            "40: error = 0.582826\n",
            "50: error = 0.494859\n",
            "60: error = 0.432585\n",
            "70: error = 0.385895\n",
            "80: error = 0.349427\n",
            "90: error = 0.320056\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This file and its tests are individualized for NetID vdwavhal.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import torch as tr\n",
        "import torch\n",
        "\n",
        "def mountain1d(x):\n",
        "    \"\"\"\n",
        "    Input: float x\n",
        "    Output: floats z, dz_dx\n",
        "      z: value of -1*x**3 + 1*x**2 - 3*x - 3\n",
        "      dz_dx: value of dz/dx evaluated at input\n",
        "    Use torch to compute derivatives and then type-cast back to float\n",
        "    \"\"\"\n",
        "    def f(val):\n",
        "        ans = -3*val**3 + 3*val**2 + 3*val + 1\n",
        "        return ans\n",
        "\n",
        "    y = tr.tensor(x,requires_grad=True)\n",
        "    z = f(y)\n",
        "    z.backward()\n",
        "    dz = y.grad\n",
        "    z = float(z)\n",
        "    dz = float(dz)\n",
        "    return z,dz\n",
        "\n",
        "def robot(t1, t2):\n",
        "    \"\"\"\n",
        "    Input: floats t1, t2\n",
        "        each joint angle, in units of radians\n",
        "    Output: floats z, dz_dt1, dz_dt2\n",
        "      z: value of (x - -1)**2 + (y - 2)**2, where\n",
        "         x = 3*(cos(t1)*cos(t2) - sin(t1)*sin(t2)) + 4*cos(t1)\n",
        "         y = 3*(sin(t1)*cos(t2) + cos(t1)*sin(t2)) + 4*sin(t1)\n",
        "      dz_dt1, dz_dt2: values of dz/dt1 and dz/dt2 evaluated at input\n",
        "    Use torch to compute derivatives and then type-cast back to float\n",
        "    \"\"\"\n",
        "    def f(x,y):\n",
        "        return 3*(tr.cos(x)*tr.cos(y) - tr.sin(x)*tr.sin(y)) + 2*tr.cos(x)\n",
        "    def g(x,y):\n",
        "        return 3*(tr.sin(x)*tr.cos(y) + tr.cos(x)*tr.sin(y)) + 2*tr.sin(x)\n",
        "    def h(x,y):\n",
        "        return (x-3)**2 + (y-1)**2\n",
        "\n",
        "\n",
        "    m = tr.tensor(t1,requires_grad=True)\n",
        "    n = tr.tensor(t2,requires_grad=True)\n",
        "    z = h(f(m,n),g(m,n))\n",
        "    z.backward()\n",
        "\n",
        "\n",
        "\n",
        "    z = float(z)\n",
        "    fx = float(m.grad)\n",
        "    fy = float(n.grad)\n",
        "    # TODO: replace with your implementation\n",
        "    return z,fx,fy\n",
        "\n",
        "def neural_network(W1, W2, W3):\n",
        "    \"\"\"\n",
        "    Input: numpy arrays W1, W2, and W3 representing weight matrices\n",
        "    Output: y, e, de_dW1, de_dW2, and de_dW3\n",
        "        float y: the output of the neural network\n",
        "        float e: the squared error of the neural network\n",
        "        numpy array de_dWk: the gradient of e with respect to Wk, for k in [1, 2, 3]\n",
        "    Use torch to compute derivatives and then type-cast back to floats and numpy arrays\n",
        "    The following documentation may be helpful:\n",
        "        https://pytorch.org/docs/stable/generated/torch.tanh.html\n",
        "        https://pytorch.org/docs/stable/generated/torch.mv.html\n",
        "        https://pytorch.org/docs/stable/tensors.html\n",
        "        https://pytorch.org/docs/stable/generated/torch.Tensor.float.html#torch.Tensor.float\n",
        "        https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy\n",
        "        https://numpy.org/doc/stable/user/basics.types.html\n",
        "    For more information, consult the instructions.\n",
        "    \"\"\"\n",
        "    # TODO: replace with your implementation\n",
        "    W1t = tr.tensor(W1,requires_grad=True)\n",
        "    W2t = tr.tensor(W2,requires_grad=True)\n",
        "    W3t = tr.tensor(W3,requires_grad=True)\n",
        "\n",
        "\n",
        "\n",
        "    x1 = [1,1]\n",
        "    x2 = [1,1,-1]\n",
        "    x3 = [-1,1,1,1]\n",
        "\n",
        "\n",
        "\n",
        "    x1t = tr.tensor(x1)\n",
        "    x2t = tr.tensor(x2)\n",
        "    x3t = tr.tensor(x3)\n",
        "\n",
        "\n",
        "\n",
        "    tr.t(x1t)\n",
        "    tr.t(x2t)\n",
        "    tr.t(x3t)\n",
        "\n",
        "    # 1.9960\n",
        "    y_nw = (W1t * tr.tanh(x1t + (W2t * tr.tanh(x2t + (W3t*tr.tanh(x3t)).sum(axis=1))).sum(axis=1))).sum(axis=1)\n",
        "    err = ((y_nw-3)**2).sum()\n",
        "    err.backward()\n",
        "\n",
        "    y_nw = float(y_nw)\n",
        "    err = float(err)\n",
        "\n",
        "\n",
        "\n",
        "    # [float(t) for t in dW1f]\n",
        "    dW1f = W1t.grad\n",
        "    dW2f = W2t.grad\n",
        "    dW3f = W3t.grad\n",
        "\n",
        "    dW1f.to(torch.float32)\n",
        "    dW2f.to(torch.float32)\n",
        "    dW3f.to(torch.float32)\n",
        "\n",
        "    dW1 = dW1f.numpy()\n",
        "    dW2 = dW2f.numpy()\n",
        "    dW3 = dW3f.numpy()\n",
        "\n",
        "    fW = (dW1,dW2,dW3)\n",
        "\n",
        "    return (y_nw, err) + fW\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # start with small random weights\n",
        "    W1 = np.random.randn(1,2).astype(np.float32) * 0.00\n",
        "\n",
        "    W2 = np.random.randn(2,3).astype(np.float32) * 0.00\n",
        "\n",
        "    W3 = np.random.randn(3,4).astype(np.float32) * 0.00\n",
        "\n",
        "    # do several iterations of gradient descent\n",
        "    for step in range(100):\n",
        "\n",
        "        # evaluate loss and gradients\n",
        "        y, e, dW1, dW2, dW3 = neural_network(W1, W2, W3)\n",
        "        if step % 10 == 0: print(\"%d: error = %f\" % (step, e))\n",
        "\n",
        "        # take step\n",
        "        eta = .1/(step + 1)\n",
        "        W1 -= dW1 * eta\n",
        "        W2 -= dW2 * eta\n",
        "        W3 -= dW3 * eta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9a_fe6aUPj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}